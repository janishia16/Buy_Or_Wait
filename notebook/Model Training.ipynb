{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cb1e3f-d997-41f6-9d0b-4c976aec39c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Todays Data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date,col\n",
    "\n",
    "today_df = spark.table(\"layers.silver.book_master\") \\\n",
    "    .filter(col(\"scrape_date\") == current_date())\n",
    "\n",
    "\n",
    "today_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a2fcda-0f90-4077-bbcd-8e4d98e4ea36",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model training"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, lag, col, when, lead\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.set_experiment(\"/Workspace/Users/jennoronha28@gmail.com/Buy_Wait\") \n",
    "\n",
    "# 1. CHECK BASE DATA FIRST\n",
    "\n",
    "base_df = spark.table(\"layers.silver.book_master\")\n",
    "print(f\"Total records in base table: {base_df.count()}\")\n",
    "print(\"Schema:\")\n",
    "base_df.printSchema()\n",
    "\n",
    "\n",
    "# 2. FEATURE ENGINEERING\n",
    "\n",
    "w_avg = (Window\n",
    "            .partitionBy(\"source\", \"book_name\")      \n",
    "            .orderBy(\"scrape_date\")\n",
    "            .rowsBetween(-7, -1))\n",
    "\n",
    "w_lag = (Window\n",
    "            .partitionBy(\"source\", \"book_name\")\n",
    "            .orderBy(\"scrape_date\"))\n",
    "\n",
    "# Create features step by step (easier to debug)\n",
    "feature_df = base_df \\\n",
    "    .withColumn(\"avg_7d_price\", avg(\"price\").over(w_avg)) \\\n",
    "    .withColumn(\"lag_1_price\", lag(\"price\", 1).over(w_lag)) \\\n",
    "    .withColumn(\"lag_7_price\", lag(\"price\", 7).over(w_lag)) \\\n",
    "    .withColumn(\"pct_change_1d\", (col(\"price\") - col(\"lag_1_price\")) / col(\"lag_1_price\") * 100) \\\n",
    "    .withColumn(\"pct_change_7d\", (col(\"price\") - col(\"lag_7_price\")) / col(\"lag_7_price\") * 100) \\\n",
    "    .withColumn(\"pct_vs_avg\", (col(\"price\") - col(\"avg_7d_price\")) / col(\"avg_7d_price\") * 100) \\\n",
    "    .withColumn(\"future_price\", lead(\"price\", 7).over(w_lag)) \\\n",
    "    .withColumn(\"label\", when(col(\"future_price\") > col(\"price\"), 1).otherwise(0))\n",
    "\n",
    "# Check feature_df\n",
    "print(f\"\\nFeature DF record count: {feature_df.count()}\")\n",
    "feature_df.select(\"book_name\", \"source\", \"scrape_date\", \"price\", \"avg_7d_price\", \"lag_7_price\", \"label\").show(5)\n",
    "\n",
    "\n",
    "# 3. PREPARE TRAINING DATA (filter nulls)\n",
    "feature_columns = [\"price\", \"avg_7d_price\", \"lag_1_price\", \"lag_7_price\", \n",
    "                   \"pct_change_1d\", \"pct_change_7d\", \"pct_vs_avg\"]\n",
    "\n",
    "train_feature_df = feature_df.dropna(subset=feature_columns + [\"label\"])\n",
    "print(f\"\\nTraining records (after dropna): {train_feature_df.count()}\")\n",
    "\n",
    "# Convert to Pandas\n",
    "train_df = train_feature_df.select(feature_columns + [\"label\"]).toPandas()\n",
    "\n",
    "X = train_df[feature_columns]\n",
    "y = train_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(f\"Label distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "\n",
    "# 4. TRAIN AND LOG MODEL\n",
    "\n",
    "catalog_name = \"layers\"\n",
    "schema_name = \"gold\"\n",
    "model_name = \"buy_wait_model\"\n",
    "registered_model_name = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"buy_wait_classifier\") as run:\n",
    "    mlflow.set_tag(\"created_by\", \"Janishia\")\n",
    "    mlflow.set_tag(\"model_type\", \"classification\")\n",
    "    mlflow.set_tag(\"description\", \"Buy and Wait Model for Book Prices\")\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_param(\"features\", str(feature_columns))\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        rf,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=registered_model_name,\n",
    "        input_example=X_train.head(1)\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Model registered: {registered_model_name}\")\n",
    "    print(f\"âœ“ Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"âœ“ Run ID: {run.info.run_id}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"WAIT\", \"BUY\"]))\n",
    "\n",
    "\n",
    "# 5. MAKE PREDICTIONS - FIXED TO GET CORRECT PRICE & URL\n",
    "\n",
    "model_uri = f\"models:/{registered_model_name}/1\"\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "# Get today's data - select columns EXPLICITLY to ensure correct values\n",
    "target_date = current_date()\n",
    "\n",
    "today_features = (feature_df\n",
    "    .filter(col(\"scrape_date\") == target_date)\n",
    "    .filter(col(\"avg_7d_price\").isNotNull())  # Only rows with enough history\n",
    "    .filter(col(\"lag_7_price\").isNotNull())\n",
    "    .select(\n",
    "        col(\"book_name\"),\n",
    "        col(\"source\"), \n",
    "        col(\"url\"),\n",
    "        col(\"price\").alias(\"current_price\"),  # Rename to be clear this is today's price\n",
    "        col(\"avg_7d_price\"),\n",
    "        col(\"lag_1_price\"),\n",
    "        col(\"lag_7_price\"),\n",
    "        col(\"pct_change_1d\"),\n",
    "        col(\"pct_change_7d\"),\n",
    "        col(\"pct_vs_avg\")\n",
    "    )\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "if len(today_features) > 0:\n",
    "    # Create feature matrix for prediction (needs 'price' column name for model)\n",
    "    X_predict = today_features[[\"current_price\", \"avg_7d_price\", \"lag_1_price\", \"lag_7_price\", \n",
    "                                 \"pct_change_1d\", \"pct_change_7d\", \"pct_vs_avg\"]].copy()\n",
    "    X_predict.columns = feature_columns  # Rename to match model's expected columns\n",
    "    \n",
    "    # Predict\n",
    "    predictions = loaded_model.predict(X_predict)\n",
    "    probabilities = loaded_model.predict_proba(X_predict)\n",
    "    \n",
    "    today_features[\"recommendation\"] = [\"ðŸŸ¢ BUY\" if p == 1 else \"ðŸ”´ WAIT\" for p in predictions]\n",
    "    today_features[\"confidence\"] = [f\"{max(prob)*100:.1f}%\" for prob in probabilities]\n",
    "    \n",
    "    # Display results\n",
    "    result_df = today_features[[\n",
    "        \"book_name\", \"source\", \"url\", \"current_price\", \n",
    "        \"avg_7d_price\", \"pct_vs_avg\", \"recommendation\", \"confidence\"\n",
    "    ]]\n",
    "    \n",
    "    print(f\"\\nðŸ“š Recommendations for {target_date}:\")\n",
    "    display(spark.createDataFrame(result_df))\n",
    "    \n",
    "    # Also show a nice summary\n",
    "    print(\"\\nðŸ“Š Summary by Book:\")\n",
    "    for _, row in result_df.iterrows():\n",
    "        price_status = \"below avg âœ“\" if row[\"pct_vs_avg\"] < 0 else \"above avg\"\n",
    "        print(f\"  {row['book_name']} ({row['source']}): â‚¹{row['current_price']:.0f} ({price_status}) â†’ {row['recommendation']} ({row['confidence']})\")\n",
    "        print(f\"    URL: {row['url']}\")\n",
    "else:\n",
    "    print(f\"No data for {target_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5220214-3bd7-4532-a5b0-005712c24302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_recommendations(target_date=None):\n",
    "    \"\"\"Get buy/wait recommendations for a specific date\"\"\"\n",
    "    from pyspark.sql.functions import current_date, max as spark_max\n",
    "    \n",
    "    if target_date is None:\n",
    "        target_date = feature_df.select(spark_max(\"scrape_date\")).collect()[0][0]\n",
    "    \n",
    "    print(f\"ðŸ“… Getting recommendations for: {target_date}\\n\")\n",
    "    \n",
    "    today_features = (feature_df\n",
    "        .filter(col(\"scrape_date\") == target_date)\n",
    "        .filter(col(\"avg_7d_price\").isNotNull())\n",
    "        .filter(col(\"lag_7_price\").isNotNull())\n",
    "        .select(\n",
    "            col(\"book_name\"), col(\"source\"), col(\"url\"),\n",
    "            col(\"price\").alias(\"current_price\"),\n",
    "            col(\"avg_7d_price\"), col(\"lag_1_price\"), col(\"lag_7_price\"),\n",
    "            col(\"pct_change_1d\"), col(\"pct_change_7d\"), col(\"pct_vs_avg\")\n",
    "        )\n",
    "        .toPandas()\n",
    "    )\n",
    "    \n",
    "    if len(today_features) == 0:\n",
    "        print(\"No data for this date\")\n",
    "        return\n",
    "    \n",
    "    X_predict = today_features[[\"current_price\", \"avg_7d_price\", \"lag_1_price\", \"lag_7_price\", \n",
    "                                 \"pct_change_1d\", \"pct_change_7d\", \"pct_vs_avg\"]].copy()\n",
    "    X_predict.columns = feature_columns\n",
    "    \n",
    "    predictions = loaded_model.predict(X_predict)\n",
    "    probabilities = loaded_model.predict_proba(X_predict)\n",
    "    \n",
    "    today_features[\"recommendation\"] = [\"ðŸŸ¢ BUY\" if p == 1 else \"ðŸ”´ WAIT\" for p in predictions]\n",
    "    today_features[\"confidence\"] = [f\"{max(prob)*100:.1f}%\" for prob in probabilities]\n",
    "    \n",
    "    # Show BUY recommendations first\n",
    "    for _, row in today_features.sort_values(\"recommendation\", ascending=False).iterrows():\n",
    "        status = \"below avg âœ“\" if row[\"pct_vs_avg\"] < 0 else \"above avg\"\n",
    "        print(f\"{row['recommendation']} {row['book_name']} ({row['source']}): â‚¹{row['current_price']:.0f} ({status}) - {row['confidence']}\")\n",
    "        print(f\"   {row['url']}\\n\")\n",
    "\n",
    "# Run it\n",
    "get_recommendations()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Model Training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
